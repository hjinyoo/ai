OpenAI Whisper API를 활용한 음성-텍스트 변환 튜토리얼 (2025년 3월 기준)
OpenAI의 Whisper 모델은 사람의 음성을 높은 정확도로 텍스트로 변환하는 자동 음성 인식(ASR) 모델
2025년 현재 Whisper는 OpenAI API를 통해 제공되며, 음성 파일을 원어 그대로 **텍스트로 필사(transcription)**하거나 영어로 **번역(translation)**하는 기능을 제공


Whisper API 소개 및 기본 음성 → 텍스트 변환
OpenAI Whisper API는 두 가지 주요 **엔드포인트(endpoint)**를 제공:

- Transcriptions 엔드포인트: 제공한 음성 파일을 해당 음성의 원어로 필사(글로 변환) 합니다 (예: 한국어 음성을 입력하면 한국어 텍스트로 출력).
- Translations 엔드포인트: 제공한 음성 파일을 영어로 번역하여 텍스트로 반환

Whisper API는 현재 Whisper v2 대형 모델(whisper-1로 식별)을 사용하며, MP3, MP4, WAV 등 다양한 음성 파일 형식을 지원
다만 한 번 요청할 수 있는 파일 크기는 최대 25MB로 제한되어 있으므로 긴 오디오의 경우 분할하여 처리


기본 사용법: 음성 파일 필사 요청
1. 오디오 파일 준비: 변환하고자 하는 음성 파일의 경로를 지정
Whisper API는 여러 형식의 오디오를 지원하는데, 일반적으로 wav나 mp3를 많이 사용
2. API 요청 구성: OpenAI Python SDK의 client.audio.transcriptions.create 메서드를 사용하여 음성 필사 요청

주요 파라미터:
- file: 열어둔 오디오 파일 객체 ("rb" 모드로 연 파일),
- model: 사용할 모델 ID ("whisper-1"로 지정하여 Whisper 모델 사용).
기본적으로 Whisper API는 응답으로 JSON 형식 데이터를 반환하며, 그 안에 "text" 필드로 변환된 텍스트를 제공
response_format 파라미터를 사용해 응답 형식을 변경할 수 있는데, 기본값 "json" 외에 "text", "srt" 등 다양한 옵션 존재

3. 응답 처리: Whisper API는 음성 내용을 텍스트로 변환하여 반환
response_format="text"로 요청한 경우 변환된 텍스트 문자열을 직접 반환하므로, 이를 출력하거나 변수에 저장 가능

ex1)
# 1. 음성 파일 열기
audio_file = open('data/speech.mp3', 'rb')
# 2. whisper API 음성 필사 요청
response = client.audio.transcriptions.create(
    file=audio_file,
    model="whisper-1",
    response_format="text"
)
# 3. 필사된 text 출력
print(response)
audio_file.close()

ex2)
with open('data/speech.mp3', 'rb') as audio_file:
    response = client.audio.transcriptions.create(
    file=audio_file,
    model="whisper-1",
    response_format="text"
)
print(response)

코드가 실행되면, speech.mp3 파일의 음성이 텍스트로 변환되어 response에 담기고, 그 내용을 출력


다양한 언어의 음성 인식 및 언어감지
Whisper 모델은 다국어 음성 인식을 지원
90개가 넘는 언어로 훈련되어 다양한 언어의 음성을 텍스트로 변환가능
기본적으로 API에 언어를 명시하지 않으면 Whisper가 음성의 언어를 자동으로 감지하여 해당 언어로 텍스트를 필사
예를 들어, 한국어 음성을 입력하면 자동으로 한국어로 텍스트를 출력

언어 감지 및 지정 (language 파라미터):

자동 감지가 대부분 잘 동작하지만, 경우에 따라 잘못된 언어로 인식될 가능성 존재 
Whisper API는 이런 경우를 대비해 language 파라미터로 입력 음성의 언어를 직접 지정하는 기능을 제공
language 파라미터에 ISO 언어 코드를 설정하면 모델이 해당 언어로 인식하도록 강제 가능

예를 들어, 한국어 음성 파일을 확실히 한국어로 인식시키고 싶다면 language="ko"로 지정가능
영어라면 language="en", 스페인어라면 language="es" 등으로 언어 코드를 설정


고급 기능 1: 타임스탬프 포함 자막 및 세부 출력
기본적인 Whisper API 응답은 전체 음성을 하나의 텍스트로 반환하지만, 경우에 따라 문장별로 시간 구간을 표시하거나 단어별 타임스탬프가 필요한 경우 존재 (예: 동영상 자막 생성, 정교한 음성 분석 등). Whisper API는 이러한 요구에 맞게 여러 가지 출력 형식을 제공

다양한 출력 형식 (response_format 옵션)
response_format 파라미터를 조정하여 Whisper의 출력 형식을 바꿀 수 있음:

"json" (기본값): { "text": "..." } 형태의 JSON 응답. 추가 메타정보 없이 최종 텍스트만 포함
"text": 순수 텍스트로 변환 결과만 반환. 
"srt": SubRip 자막 형식으로 반환. 자막처럼 각 문장에 시간 코드가 포함
"vtt": WebVTT 자막 형식으로 반환. (srt와 유사한 자막 표준 형식)
"verbose_json": 인식 결과에 대한 상세 정보를 담은 JSON으로 반환. 인식된 문장별 세그먼트와 타임스탬프, 확률 등 메타데이터가 모두 포함
자막 형식 출력: 예를 들어 음성 파일을 response_format="srt"으로 요청하면, Whisper는 출력 텍스트를 일정 길이로 나누어 각 부분에 시작-끝 시간이 포함된 자막 형식으로 돌려줌

고급 기능 2: 실시간 음성 변환 (마이크 입력 처리)
실시간 처리란 말 그대로 사용자가 말하는 동시에(text) 바로바로 텍스트로 변환하는 것을 의미
Whisper API는 스트리밍 엔드포인트를 제공하지는 않으나, 짧은 구간으로 녹음하여 연속으로 API에 보내는 방식으로 유사 실시간 처리가 가능

주의: 네트워크 API를 이용하는 이상 완벽한 실시간(동시에 변환) 처리에는 한계 존재
음성을 일정 간격으로 끊어서 전송하면 약간의 지연은 발생하지만, 빠른 응답 속도를 감안하면 거의 실시간에 가까운 자막을 구현
진정한 실시간 처리가 필요하다면 오픈소스 Whisper 모델을 로컬에서 사용하거나 스트리밍 기능을 지원하는 서비스를 검토
마이크 입력 녹음하여 전송하기
Python에서는 sounddevice나 pyaudio 등의 라이브러리를 사용하여 마이크로부터 오디오 데이터를 받을 수 있음


짧은 음성 녹음: sounddevice.rec 함수를 사용하면 지정한 초 만큼 마이크로부터 오디오를 녹음할 수 있습니다. 아래 예시는 5초간 모노 음성을 16kHz로 녹음합니다.
오디오 데이터를 파일로 저장: Whisper API에 전송하려면 오디오 데이터를 파일 형태 (또는 파일 객체)로 제공해야 합니다. scipy.io.wavfile.write 함수를 이용해 녹음한 데이터를 WAV 파일로 저장합니다.
저장한 파일 전송: 앞서와 동일하게 client.audio.transcriptions.create를 사용하여 Whisper API로 WAV 파일을 보내면 됩니다.
코드 예시는 다음과 같습니다 (5초간 녹음 후 즉시 전송):

허깅페이스
Inference API 이용 : 모델의 결과를 server에서
pipeline() 이용 : 모델을 다운로드 받아 모델의 결과를 local에서
raw text -> tokenizer -> model -> [0.11, 0.55, 0.xx, ~] logits값으로 prediction결과 출력
허깅페이스 transformers에서 지원하는 task

※ 가상환경 구성
pip install torch==2.6.0 torchvision==0.21.0 --index-url https://download.pytorch.org/whl/cpu 
pip install sympy==1.13.1 fsspec # 2025.9.0
pip install transformers #4.57.3 허깅페이스 모델 사용
pip install sentencepiece  # Tokenizer 기능(구글作)
pip install hf_xet             # 대용량 허깅페이스 파일 업로드/다운로드
pip install sacremoses      # 한영번역의 경고를 막고, 번역 품질을 높이기 위함

c:/사용자/내컴퓨터명/.cache/huggingface/hub 모델 다운로드

from transformers import pipeline


1. 텍스트 기반 감정분석(긍정/부정)
1) "sentiment-analysis" : "text-classification"의 별칭(감정분석 전용으로 사용) 
    객체명 = pipeline(task="sentiment-analysis")
    객체명("분석할 내용")
2) "text-classification" : 감정분석, 뉴스분류, 리뷰분류 등 일반적인 문장 분류
    객체명 = pipeline(task="text-classification", model="distilbert/distilbert-base-uncased-finetuned-sst-2-english")
    객체명("분석할 내용")

2. 제로샷분류(Zero-shot분류)
기계학습 및 자연어처리에서 각 개별 작업에 대한 특정 교육 없이 작업을 수행할 수 있는 모형(비지도 학습)
"zero-shot-classification" : 레이블을 학습 없이 주어진 후보군 중에서 분류
1) 객체명 = pipeline(task="zero-shot-classification",  model="facebook/bart-large-mnli")
   객체명("분석할 내용", candidate_labels=["label1", "label2", "label3", "...."])
2)객체명 = pipeline(task="zero-shot-classification",  model="facebook/bart-large-mnli")
분석할 내용의 변수명 = "분석할 내용"
candidate_labels = ["label1", "label2", "label3", "...."]
객체명(분석할 내용의 변수명, candidate_labels)

3. text 생성
"text-generation" : 텍스트 생성 (GPT류 모델에 사용)
"text2text-generation" : 번역, 요약, 등 입력 -> 출력 변환
1) 객체명 = pipeline("text-generation", "gpt2")  # 텍스트 생성 gpt3부터는 허깅페이스없음
   객체명(
      "분석할 내용",
      pad_token_id = generation.tokenizer.eos_token_id
) # pad_token_id 경고를 없애려고 setting
2) 객체명 = pipeline("text-generation", "skt/kogpt2-base-v2")
    객체명("분석할 내용 ",
                    pad_token_id = generation.tokenizer.eos_token_id,
                    max_new_tokens = n, # 생성할 최대 길이(생성할 토큰 수)
                    num_return_sequences=n, # 생성할 문장 갯수
                    do_sample=True, # 다양한 샘플 사용
                    top_k=n,       # top_k 샘플링(확률 높은 상위 50개 토큰만 사용)
                    top_p=0.n,      # 확률이 높은 순서대로 n%가 될 때까지의 단어들로만 후보로 사용
                    temperature=n, # 창의성 조절(낮을수록 보수적)
                    no_repeat_ngram_size=n # 반복방지
                   )

4. 마스크(빈칸) 채우기
"fill-mask" : 빈칸 채우기
객체명 = pipeline(task='fill-mask',
                    model='distilbert/distilroberta-base') # 마스크 채우기
객체명("<mask>포함 분석할 내용")
※ InferenceAPI 사용
from dotenv import load_dotenv
import os
load_dotenv()
# os.environ['HF_TOKEN']
# 허깅페이스 토큰을 READ권한으로 생성하여 .env에 추가 (.git ignore파일 추가)
from dotenv import load_dotenv
import os
load_dotenv()
from huggingface_hub import InferenceClient
객체명 = InferenceClient(
            provider="hf-inference",
            api_key=os.environ['HF_TOKEN'], # 허깅페이스 토큰 키
)
객체명.fill_mask(
    "[MASK] 포함한 분석할 내용",
     model="google-bert/bert-base-uncased",
     top_k=n #기본 5개
)

5. 개체명 인식(NER : Named Entity Recognition)
"ner" : "token-classification"의 별칭
"token-classification" : 개체명 인식(NER ; Named Entity Recognition) 등 단위 라벨링
객체명 = pipeline(task="ner",
               model="dbmdz/bert-large-cased-finetuned-conll03-english",
               grouped_entities=True) # 개체들을 그룹으로 묶을지 말지
객체명("분석할 내용")

6. 질의 응답
"question-answering" : 주어진 context를 보고 질문에 답하기
1) 객체명 = pipeline("question-answering",
                            "distilbert/distilbert-base-cased-distilled-squad") # 질의응답
    객체명(
       question="질의할 내용",
       context = "분석할 내용"
     ) # 지문(context)을 이용해서 답하기
2) 객체명 = pipeline("question-answering",
                            "distilbert/distilbert-base-cased-distilled-squad") # 질의응답
    context = "분석할 내용"
   객체명(question="질의할 내용", context=context)

7. 문서요약
현재 torch 버전이 2.6이하이면 Hugging Face에서 강제로 막고 있음
"summarization" : 텍스트 요약
객체명 = pipeline(task='summarization',
                      model='sshleifer/distilbart-cnn-12-6')
객체명(
  """요약할 내용""",
    max_length=n, # 요약할 내용의 최대 토큰 수
    min_length=n,  # 요약할 내용의 최소 토큰 수
    do_sample=False # 랜덤성이 없음/항상 비슷한 요약
)

8. 번역
pip install sacremoses : 한영번역에서의 경고를 줄이고, 번역품질을 높이기 위해
"translation" : 번역
객체명 = pipeline(task="translation",
                      model='번역할 모델명')
객체명("번역할 내용")

9. 이미지를 설명하는 텍스트 생성
"image-to-text" : 그림을 설명
"image-classification" : 이미지 분류

객체명 = pipeline(task="image-to-text",
                     model="ydshieh/vit-gpt2-coco-en")
url = "이미지 url주소"
객체명(url, max_new_tokens=n) # 최대 n토큰으로 이미지에 대해 설명하는 text







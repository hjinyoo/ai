Attention(스마트번역기)
- Google Neural Machine Translation(GNMT)
- RNN기반의 Sequence to Sequence 방식
- 인코더(입력) / 디코더(모범출력) 연결구조
- 응용분야:자연어



from tensorflow.keras.layers import Attention, Concatenate
ex)
# 인코더 LSTM 구현
ENC_IN = Input(shape=(n, alpha_total_size))
# 윗출력, state_c, state_h
ENC_OUT, state_h, state_c = LSTM(
                    units=MY_HIDDEN,
                    return_sequences=True,
                    return_state=True, 
            )(ENC_IN)
# 인코더와 디코더를 연결할 link
link = [state_h, state_c]
# 디코더 구현 : return_sequences=True 위로 올라가는 출력값 사용
DEC_IN = Input(shape=(3, alpha_total_size))
DEC_MID, _, _ = LSTM(units=MY_HIDDEN,
               return_state=True,
               return_sequences=True)(DEC_IN,
                                      initial_state=link)
# 어텐션 메커니즘
CONTEXT_VECTOR = Attention()([DEC_MID, ENC_OUT])

# 컨텍스트벡터와 디코더 LSTM출력을 결합
CONTEXT_AND_LSTM_OUT = Concatenate()([CONTEXT_VECTOR, DEC_MID])

# 최종 출력층
OUT = Dense(units=alpha_total_size,
                activation='softmax')(CONTEXT_AND_LSTM_OUT)
# 모델
model = Model(inputs=[ENC_IN, DEC_IN],
              outputs=OUT)
model.summary()